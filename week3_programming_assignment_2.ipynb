{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sikander/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (19,47) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans=pd.read_csv('lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.drop('bad_loans',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
      "0           1        0        1        0        0        0        0        0   \n",
      "1          -1        0        0        1        0        0        0        0   \n",
      "\n",
      "   term_ 36 months  term_ 60 months  ...  emp_length_10+ years  \\\n",
      "0                1                0  ...                     1   \n",
      "1                0                1  ...                     0   \n",
      "\n",
      "   emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
      "0                   0                   0                   0   \n",
      "1                   0                   0                   0   \n",
      "\n",
      "   emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
      "0                   0                   0                   0   \n",
      "1                   0                   0                   0   \n",
      "\n",
      "   emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \n",
      "0                   0                   0                    0  \n",
      "1                   0                   0                    1  \n",
      "\n",
      "[2 rows x 25 columns]\n",
      "Index(['safe_loans', 'grade_A', 'grade_B', 'grade_C', 'grade_D', 'grade_E',\n",
      "       'grade_F', 'grade_G', 'term_ 36 months', 'term_ 60 months',\n",
      "       'home_ownership_MORTGAGE', 'home_ownership_OTHER', 'home_ownership_OWN',\n",
      "       'home_ownership_RENT', 'emp_length_1 year', 'emp_length_10+ years',\n",
      "       'emp_length_2 years', 'emp_length_3 years', 'emp_length_4 years',\n",
      "       'emp_length_5 years', 'emp_length_6 years', 'emp_length_7 years',\n",
      "       'emp_length_8 years', 'emp_length_9 years', 'emp_length_< 1 year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_variables = []\n",
    "for feat_name, feat_type in zip(loans.columns, loans.dtypes):\n",
    "    if feat_type == object:\n",
    "        categorical_variables.append(feat_name)\n",
    "        \n",
    "for feature in categorical_variables:\n",
    "    \n",
    "    loans_one_hot_encoded = pd.get_dummies(loans[feature],prefix=feature)\n",
    "    #print loans_one_hot_encoded\n",
    "    \n",
    "    loans = loans.drop(feature, axis=1)\n",
    "    for col in loans_one_hot_encoded.columns:\n",
    "        loans[col] = loans_one_hot_encoded[col]\n",
    "    \n",
    "print (loans.head(2))        \n",
    "print (loans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('module-5-assignment-2-train-idx.json') as train_data_file:    \n",
    "    train_idx  = json.load(train_data_file)\n",
    "with open('module-5-assignment-2-test-idx.json') as test_data_file:    \n",
    "    test_idx = json.load(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loans.iloc[train_idx]\n",
    "test_data = loans.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0    \n",
    "    safe_loan = (labels_in_node==1).sum()\n",
    "    # Count the number of -1's (risky loans)\n",
    "    ## YOUR CODE HERE                \n",
    "    risky_loan = (labels_in_node==-1).sum()\n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    ## YOUR CODE HERE    \n",
    "    return min(safe_loan, risky_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    target_values = data[target]\n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(data))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature] == 0]\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        ## YOUR CODE HERE\n",
    "        right_split = data[data[feature] == 0]\n",
    "            \n",
    "        left_mistakes = intermediate_node_num_mistakes(left_split[target])            \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        ## YOUR CODE HERE\n",
    "        right_mistakes = intermediate_node_num_mistakes(right_split[target])  \n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_mistakes + right_mistakes) / num_data_points\n",
    "\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        ## YOUR CODE HERE\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values):    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True }   ## YOUR CODE HERE \n",
    "   \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])    \n",
    "\n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1         ## YOUR CODE HERE\n",
    "    else:\n",
    "        leaf['prediction'] = -1         ## YOUR CODE HERE        \n",
    "\n",
    "    # Return the leaf node\n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print (\"--------------------------------------------------------------------\")\n",
    "    print (\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node.\n",
    "    # Recall you wrote a function intermediate_node_num_mistakes to compute this.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  ## YOUR CODE HERE\n",
    "        print (\"Stopping condition 1 reached.\")     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2 (check if there are remaining features to consider splitting on)\n",
    "    if remaining_features == []:   ## YOUR CODE HERE\n",
    "        print (\"Stopping condition 2 reached.\")    \n",
    "        # If there are no remaining features to consider, make current node a leaf node\n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth >= max_depth:  ## YOUR CODE HERE\n",
    "        print (\"Reached maximum depth. Stopping for now.\")\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Find the best splitting feature (recall the function best_splitting_feature implemented above)\n",
    "    ## YOUR CODE HERE\n",
    "    splitting_feature = best_splitting_feature(data, remaining_features, target)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]      ## YOUR CODE HERE\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    print (\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    if len(right_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        ## YOUR CODE HERE\n",
    "        return create_leaf(right_split[target])\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, current_depth + 1, max_depth)        \n",
    "    ## YOUR CODE HERE\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grade_A', 'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F', 'grade_G', 'term_ 36 months', 'term_ 60 months', 'home_ownership_MORTGAGE', 'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT', 'emp_length_1 year', 'emp_length_10+ years', 'emp_length_2 years', 'emp_length_3 years', 'emp_length_4 years', 'emp_length_5 years', 'emp_length_6 years', 'emp_length_7 years', 'emp_length_8 years', 'emp_length_9 years', 'emp_length_< 1 year']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = list(train_data.columns)\n",
    "a.remove('safe_loans')\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature term_ 60 months. (0, 9223)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature home_ownership_RENT. (14665, 13336)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (14665 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (2416, 12249)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2416 data points).\n",
      "Split on feature home_ownership_OWN. (58, 2358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (58 data points).\n",
      "Split on feature home_ownership_OTHER. (0, 58)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2358 data points).\n",
      "Split on feature grade_C. (1730, 628)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1730 data points).\n",
      "Split on feature grade_B. (1009, 721)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1009 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (721 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (628 data points).\n",
      "Split on feature emp_length_10+ years. (465, 163)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (163 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (12249 data points).\n",
      "Split on feature grade_B. (7906, 4343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (7906 data points).\n",
      "Split on feature grade_C. (4987, 2919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4987 data points).\n",
      "Split on feature grade_D. (3333, 1654)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3333 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1654 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2919 data points).\n",
      "Split on feature emp_length_10+ years. (1943, 976)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1943 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (976 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (4343 data points).\n",
      "Split on feature emp_length_10+ years. (2846, 1497)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2846 data points).\n",
      "Split on feature emp_length_5 years. (2503, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2503 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1497 data points).\n",
      "Split on feature grade_A. (1497, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (13336 data points).\n",
      "Split on feature grade_B. (9099, 4237)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9099 data points).\n",
      "Split on feature grade_C. (5436, 3663)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5436 data points).\n",
      "Split on feature grade_D. (2803, 2633)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2803 data points).\n",
      "Split on feature grade_A. (979, 1824)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (979 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1824 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2633 data points).\n",
      "Split on feature emp_length_10+ years. (2143, 490)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2143 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (490 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3663 data points).\n",
      "Split on feature emp_length_10+ years. (3009, 654)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3009 data points).\n",
      "Split on feature emp_length_2 years. (2551, 458)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2551 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (458 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (654 data points).\n",
      "Split on feature grade_A. (654, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4237 data points).\n",
      "Split on feature emp_length_10+ years. (3459, 778)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3459 data points).\n",
      "Split on feature emp_length_< 1 year. (2946, 513)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2946 data points).\n",
      "Split on feature emp_length_2 years. (2450, 496)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2450 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (496 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (513 data points).\n",
      "Split on feature grade_A. (513, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (778 data points).\n",
      "Split on feature grade_A. (778, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree = decision_tree_create(train_data, a, 'safe_loans', current_depth = 0, max_depth = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):\n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "             print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "             print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            ### YOUR CODE HERE\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe_loans                -1\n",
      "grade_A                    0\n",
      "grade_B                    0\n",
      "grade_C                    0\n",
      "grade_D                    1\n",
      "grade_E                    0\n",
      "grade_F                    0\n",
      "grade_G                    0\n",
      "term_ 36 months            0\n",
      "term_ 60 months            1\n",
      "home_ownership_MORTGAGE    0\n",
      "home_ownership_OTHER       0\n",
      "home_ownership_OWN         0\n",
      "home_ownership_RENT        1\n",
      "emp_length_1 year          0\n",
      "emp_length_10+ years       0\n",
      "emp_length_2 years         1\n",
      "emp_length_3 years         0\n",
      "emp_length_4 years         0\n",
      "emp_length_5 years         0\n",
      "emp_length_6 years         0\n",
      "emp_length_7 years         0\n",
      "emp_length_8 years         0\n",
      "emp_length_9 years         0\n",
      "emp_length_< 1 year        0\n",
      "Name: 24, dtype: int64\n",
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "print (test_data.iloc[0])\n",
    "print ('Predicted class: %s ' % classify(my_decision_tree, test_data.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on term_ 36 months = 0\n",
      "At leaf, predicting -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, test_data.iloc[0], annotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis=1)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    return (data['safe_loans'] != np.array(prediction)).values.sum() *1. / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3853942266264541"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(my_decision_tree, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print (\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_',1)\n",
    "    print ('                       %s' % name)\n",
    "    print ('         |---------------|----------------|')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('  [{0} == 0]               [{0} == 1]    '.format(split_name))\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]               [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a47ee4abee91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_stump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_decision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_decision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'splitting_feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-8b7b169c5483>\u001b[0m in \u001b[0;36mprint_stump\u001b[0;34m(tree, name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_stump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'root'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msplit_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'splitting_feature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# split_name is something like 'term. 36 months'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msplit_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"(leaf, label: %s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
